#LyX 2.0 created this file. For more info see http://www.lyx.org/
\lyxformat 413
\begin_document
\begin_header
\textclass article
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman default
\font_sans default
\font_typewriter default
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100

\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry true
\use_amsmath 1
\use_esint 1
\use_mhchem 1
\use_mathdots 1
\cite_engine basic
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 1in
\topmargin 1in
\rightmargin 1in
\bottommargin 1in
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Project Report
\end_layout

\begin_layout Author
Nicolas Voirol (186268), Damien Firmenich(178474)
\begin_inset Newline newline
\end_inset

EPFL, School of Computer and Communication Sciences
\end_layout

\begin_layout Date
Spring 2013
\end_layout

\begin_layout Section
Introduction
\end_layout

\begin_layout Standard
Pattern classification and machine learning provide us with a large set
 of powerful tools and techniques.
 The sheer number and the diversity of these tools can make selecting the
 ideal technique for a given problem quite difficult.
 We could simply decide we want to use the best techniques available to
 make sure our system is optimal, but with complexity comes time consumption.
 Therefore, when using an elaborate tool to solve a classification problem,
 it is generally useful to make sure it actually performs better than a
 simpler technique.
 This project compares the Multi Layer Preceptron technique against Least
 Squares Estimation and Logistic Regression on the NORB dataset.
\end_layout

\begin_layout Section*
Methods
\end_layout

\begin_layout Standard
The theory places gradient descent in batch mode above stochastic online
 descent in terms of the optimality of each update.
 We therefore implemented partial (or mini) batch mode with varying batch
 size to compare their actual impact on convergence speed and final error
 rate.
 We implemented this feature in our MLP and Logistic Regression classifiers
 since they are both based on gradient descent.
\end_layout

\begin_layout Subsection*
Model and parameters
\end_layout

\begin_layout Subsection
Binary MLP and multi-way MLP
\end_layout

\begin_layout Paragraph
Backpropagation
\end_layout

\begin_layout Standard
The vectorized backpropagation equations that we implemented for the binary
 MLP are as follows:
\end_layout

\begin_layout Standard
The residuals are computed using the derivative of the 
\begin_inset Formula $tanh$
\end_inset

 tranfer function, and the derivative of the logistic loss function.
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
\mathbf{r}^{\left(3\right)} & = & \sigma\left(\mathbf{a}^{\left(3\right)}\right)-\frac{1}{2}\left(\mathbf{t}+1\right)\\
\mathbf{r}_{L}^{\left(2\right)} & = & \mathbf{a}_{LR}^{\left(2\right)}\sigma'\left(\mathbf{a}_{L}^{\left(2\right)}\right)\sigma\left(\mathbf{a}_{R}^{\left(2\right)}\right)\mathbf{w}^{\left(3\right)}\mathbf{r}^{\left(3\right)}\\
\mathbf{r}_{R}^{\left(2\right)} & = & \mathbf{a}_{LR}^{\left(2\right)}\sigma\left(\mathbf{a}_{L}^{\left(2\right)}\right)\sigma'\left(\mathbf{a}_{R}^{\left(2\right)}\right)\mathbf{w}^{\left(3\right)}\mathbf{r}^{\left(3\right)}\\
\mathbf{r}_{LR}^{\left(3\right)} & = & \sigma\left(\mathbf{a}_{L}^{\left(2\right)}\right)\sigma\left(\mathbf{a}_{R}^{\left(2\right)}\right)\mathbf{w}^{\left(3\right)}\mathbf{r}^{\left(3\right)}\\
\mathbf{r}_{L}^{\left(1\right)} & = & tanh'\left(\mathbf{a}_{L}^{\left(2\right)}\right)\left(\left(\mathbf{w}_{L}^{\left(2\right)}\right)^{T}\mathbf{r}_{L}^{\left(2\right)}+\left(\mathbf{w}_{LR[L]}^{\left(2\right)}\right)^{T}\mathbf{r}_{LR}^{\left(2\right)}\right)\\
\mathbf{r}_{R}^{\left(1\right)} & = & tanh'\left(\mathbf{a}_{R}^{\left(2\right)}\right)\left(\left(\mathbf{w}_{L}^{\left(2\right)}\right)^{T}\mathbf{r}_{L}^{\left(2\right)}+\left(\mathbf{w}_{LR[R]}^{\left(2\right)}\right)^{T}\mathbf{r}_{LR}^{\left(2\right)}\right)
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
\nabla_{w^{\left(3\right)}}E_{i} & = & r^{\left(3\right)}z^{\left(2\right)}\left(a^{\left(2\right)}\right)\\
\nabla_{w_{L}^{\left(2\right)}}E_{i} & = & \mathbf{r}_{L}^{\left(2\right)}\mathbf{z}_{L}^{\left(1\right)}\left(\mathbf{a}_{L}^{\left(1\right)}\right)\\
\mathbf{\nabla}_{w_{R}^{\left(2\right)}}E_{i} & = & \mathbf{r}_{R}^{\left(2\right)}\mathbf{z}_{R}^{\left(1\right)}\left(\mathbf{a}_{R}^{\left(1\right)}\right)\\
\nabla_{w_{LR}^{\left(2\right)}}E_{i} & = & \mathbf{r}_{LR}^{\left(2\right)}\left[\mathbf{z}_{L}^{\left(1\right)}\,\mathbf{z}_{R}^{\left(1\right)}\right]\\
\nabla_{w_{L}^{(1)}}E_{i} & = & \mathbf{r}_{L}^{\left(1\right)}\mathbf{x}_{L}\\
\nabla_{w_{R}^{\left(1\right)}}E_{i} & = & \mathbf{r_{R}^{\left(1\right)}}\mathbf{x}_{R}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
And the gradients at each layer and for each stereo image (left, right)
 are computed using the residuals.
\end_layout

\begin_layout Standard
with 
\begin_inset Formula $w_{LR[L]}^{\left(2\right)}$
\end_inset

 representing the portion of the weight matrix corresponding to the left
 image, and analogously for 
\begin_inset Formula $w_{LR[R]}^{\left(2\right)}$
\end_inset

.
 The notation 
\begin_inset Formula $\left[\mathbf{z}_{L}^{\left(1\right)}\,\mathbf{z}_{R}^{\left(1\right)}\right]$
\end_inset

 means the concatenation of 
\begin_inset Formula $\mathbf{z}_{L}^{\left(1\right)}$
\end_inset

 and 
\begin_inset Formula $\mathbf{z}_{R}^{\left(1\right)}$
\end_inset

.
\end_layout

\begin_layout Paragraph
Multi-way classification
\end_layout

\begin_layout Standard
Most of the MLP we implemented for binary classification could be reused
 for multi-way classification.
 Two changes had to be done, first the computation of the error on the top-layer
, as we had to use the squared error, and the class selection.
\end_layout

\begin_layout Standard
The squared error induces a change for the top-layer residual which becomes:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
r^{\left(3\right)}=a^{\left(3\right)}-t
\]

\end_inset


\end_layout

\begin_layout Standard
In multi-way classification we also have 5 outputs instead of a single one,
 so the computation of the class associated with the input image is computed
 is 
\begin_inset Formula $\underset{i=1,...,5}{argmax}\left(a_{i}^{\left(3\right)}\right)$
\end_inset

 instead of 
\begin_inset Formula $sgn\left(a^{\left(3\right)}\right)$
\end_inset

.
\end_layout

\begin_layout Paragraph
Model selection
\end_layout

\begin_layout Standard
For each configuration of the MLP and gradient descent parameters, we estimated
 the total error on the validation set (a third of the initial set, selected
 randomly).
 We tested performance for the learning rate 
\begin_inset Formula $\eta$
\end_inset

, the momentum term 
\begin_inset Formula $\mu$
\end_inset

 and the size of the mini-batch.
 Specifically for MLP, we tested the numbers of activation units for the
 first and second layer (
\begin_inset Formula $H_{1}$
\end_inset

, 
\begin_inset Formula $H_{2})$
\end_inset

.
 Its unintuitive how the value of one parameter may affect the performance
 of the MLP, especially because they are not independent and so we need
 to run the algorithm on all combinations of value to get a reasonable estimatio
n.
 For the learning 
\begin_inset Formula $\eta$
\end_inset

 we choose constant functions (in the order of 
\begin_inset Formula $1e^{-3}$
\end_inset

to 
\begin_inset Formula $1e^{-5}$
\end_inset

) as well as variations of 
\begin_inset Formula $\frac{1}{x}$
\end_inset

.
 The momentum term varied from 
\begin_inset Formula $0.05$
\end_inset

 to 
\begin_inset Formula $0.25$
\end_inset

, and the mini-batch size ranged from 
\begin_inset Formula $1$
\end_inset

 to 
\begin_inset Formula $50$
\end_inset

.
 As explained in the problem statement, we tested configuration of MLP with
 
\begin_inset Formula $1$
\end_inset

 to 
\begin_inset Formula $80$
\end_inset

 activation units in each hidden layers.
\end_layout

\begin_layout Standard
The optimal parameters for the binary MLP are 
\begin_inset Formula $\eta=1e^{-3}$
\end_inset

, 
\begin_inset Formula $\mu=1e^{-1}$
\end_inset

, mini-batch size of 
\begin_inset Formula $20$
\end_inset

.
 The numbers of units in the first layer is 
\begin_inset Formula $20$
\end_inset

 and for the second layer is 
\begin_inset Formula $50$
\end_inset

.
 For the multi-way MLP, we found 
\begin_inset Formula $\mu=1e^{-1}$
\end_inset

, mini-batch size of 
\begin_inset Formula $5$
\end_inset

.
 TODO: h1, h2, eta
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename ../plots/errors_mlp5_overfitting.pdf
	width 80col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
Example of overfitting for the multi-way MLP used for early-stopping
\begin_inset CommandInset label
LatexCommand label
name "fig:Example-of-overfitting"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Logistic regression
\end_layout

\begin_layout Standard
The logistic regression technique doesn't have any model parameters, but
 it relies on gradient descent which does (namely the learning rate 
\begin_inset Formula $\eta$
\end_inset

, momentum term 
\begin_inset Formula $\mu$
\end_inset

, and mini batch size).
 Unfortunately, as in the MLP case, it is difficult to perform standard
 model selection on these parameters.
\end_layout

\begin_layout Standard
After having manually determined plausible parameter combinations, we performed
 model selection by comparing convergence speed and final error rate on
 these values.
 During this process, we realized that logistic regression was extremely
 sensitive to initialization and training order, so we ran our model selection
 multiple times on each parameter combination and considered mean and variance
 instead of individual results when comparing models.
 The different strategies that stood out were
\end_layout

\begin_layout Subsection
Least squares estimation
\end_layout

\begin_layout Paragraph
Error function
\end_layout

\begin_layout Standard
The analytical solution to the least squares estimation with Tikhonov regulariza
tion problem is obtained by solving the normal equations problem.
 In the case of multi-way classification, the solution is trivially extended
 by solving the equations for each of the k vectors obtained by using 
\emph on
1 of K
\emph default
 encoding for the target vector 
\series bold
\emph on
t
\series default
\emph default
.
 But this also means we multiply the impact of the Tikhonov regularizer
 by k.
 To compensate for this, we modified the regularized error function slightly
 :
\begin_inset Formula 
\[
E(\mathbf{W})=\frac{1}{2}\sum_{i=1}^{N}\parallel\mathbf{y}(\mathbf{x}_{i})-\mathbf{\tilde{t}}_{i}\parallel^{2}+\frac{1}{k}\frac{\nu}{2}\sum_{k=1}^{K}\parallel\mathbf{w}_{k}\parallel^{2}
\]

\end_inset


\end_layout

\begin_layout Paragraph
Model selection
\end_layout

\begin_layout Standard
The model selection process for the least squares estimation technique was
 by far the simplest since we were dealing with a single parameter and a
 constant convergence time.
 All computations were performed using 10-fold cross-validation, so we combined
 the 10 values obtained after each parameter validation run into boxplots
 for clarity.
\end_layout

\begin_layout Standard
We initially got a feeling for plausible values by setting the Tikhonov
 regularizer to the exponential range 
\begin_inset Formula $\left\{ 0\right\} \cup\left\{ 5e^{-2}*2^{k}\mid0\leq k<10\right\} $
\end_inset

 (see Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Interval-selection-lstsq"

\end_inset

).
 Once we had singled out a suitable interval, namely 
\begin_inset Formula $\left[0;1.2\right]$
\end_inset

, we performed a more precise search by resorting to a linear parameter
 space of 
\begin_inset Formula $\left\{ 5e^{-2}*k\mid0\leq k<25\right\} $
\end_inset

 (see Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Error-function-value"

\end_inset

) and we can thus place the optimal regularizer at 
\begin_inset Formula $0.6$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename lstsq_interval.pdf
	width 80col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
Interval selection for Tikhonov regularization parameter.
 
\begin_inset CommandInset label
LatexCommand label
name "fig:Interval-selection-lstsq"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename lstsq_errors.pdf
	width 50col%

\end_inset


\begin_inset Graphics
	filename lstsq_classerrors.pdf
	width 50col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
Error function value (left) and classification error rate (right) during
 parameter selection for Tikhonov regularizer.
 We can clearly see the curve minimum around 0.5 (the actual minimum is at
 0.6).
 
\begin_inset CommandInset label
LatexCommand label
name "fig:Error-function-value"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Section
Results and Discussion
\end_layout

\begin_layout Subsection
Binary MLP and multi-way MLP
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement h
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename ../plots/errors_mlp2.pdf
	width 80col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
Logistic error for binary classification using MLP for the training and
 the validation set.
\begin_inset CommandInset label
LatexCommand label
name "fig:Logistic-error-for"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
The graph in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Logistic-error-for"

\end_inset

 shows the logistic error computed with the optimal parameters for the binary
 MLP.
 Both the errors for the training and validation set are very close and
 the MLP converges quickly to an optimal solution, as both the classification
 error for the training and the validation set are null.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement h
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename ../plots/errors_mlp5.pdf
	width 80col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\align center
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename ../plots/classerrors_mlp5.pdf
	width 80col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
Logistic error and classification error for multi-way classification using
 MLP for the training and the validation set.
\begin_inset CommandInset label
LatexCommand label
name "fig:Logistic-error-for-1"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
The graph in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Logistic-error-for-1"

\end_inset

 show the same metric for the multi-way MLP.
 The system converges slower than before and there is a better separation
 between the errors of the training and the validation set.
 We also showed a graph representing the classification error for both set,
 as they both converge to zero but not at the same speed.
 It is interesting to notice that both the binary and multi-way MLP converge
 to a classification error of zero after enough training.
\end_layout

\begin_layout Subsection
Logistic regression
\end_layout

\begin_layout Subsection
Linear regression
\end_layout

\end_body
\end_document
