#LyX 2.0 created this file. For more info see http://www.lyx.org/
\lyxformat 413
\begin_document
\begin_header
\textclass article
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman default
\font_sans default
\font_typewriter default
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100

\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\float_placement h
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry true
\use_amsmath 1
\use_esint 1
\use_mhchem 1
\use_mathdots 1
\cite_engine basic
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 1in
\topmargin 1in
\rightmargin 1in
\bottommargin 1in
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Pattern Classification and Machine Learning:
\begin_inset Newline newline
\end_inset

Project Report
\end_layout

\begin_layout Author
Nicolas Voirol (186268), Damien Firmenich(178474)
\begin_inset Newline newline
\end_inset

EPFL, School of Computer and Communication Sciences
\end_layout

\begin_layout Date
Spring 2013
\end_layout

\begin_layout Section
Introduction
\end_layout

\begin_layout Standard
Pattern classification and machine learning provide us with a large set
 of powerful tools and techniques.
 The sheer number and the diversity of these tools can make selecting the
 ideal technique for a given problem quite difficult.
 We could simply decide we want to use the best techniques available to
 make sure our system is optimal, but with complexity comes time consumption.
 Therefore, when using an elaborate tool to solve a classification problem,
 it is generally useful to make sure it actually performs better than a
 simpler technique.
 This project compares the Multi-Layer Perceptron technique against Least
 Squares Estimation and Logistic Regression on the NORB dataset.
\end_layout

\begin_layout Section*
Methods
\end_layout

\begin_layout Standard
The theory places gradient descent in batch mode above stochastic online
 descent in terms of the optimality of each update.
 We therefore implemented partial (or mini) batch mode with varying batch
 size to compare their actual impact on convergence speed and final error
 rate.
 We implemented this feature in our MLP and Logistic Regression classifiers
 since they are both based on gradient descent.
\end_layout

\begin_layout Subsection*
Model and parameters
\end_layout

\begin_layout Subsection
Binary MLP and multi-way MLP
\end_layout

\begin_layout Paragraph
Backpropagation
\end_layout

\begin_layout Standard
The vectorized backpropagation equations that we implemented for the binary
 MLP are as follows :
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
\mathbf{r}^{\left(3\right)}=\sigma\left(\mathbf{a}^{\left(3\right)}\right)-\frac{1}{2}\left(\mathbf{t}+1\right) & \Rightarrow & \nabla_{\mathbf{w}^{\left(3\right)}}E_{i}=r^{\left(3\right)}\left(z^{\left(2\right)}\right)^{T}\\
\mathbf{r}_{L}^{\left(2\right)}=\mathbf{a}_{LR}^{\left(2\right)}\sigma'\left(\mathbf{a}_{L}^{\left(2\right)}\right)\sigma\left(\mathbf{a}_{R}^{\left(2\right)}\right)\mathbf{w}^{\left(3\right)}\mathbf{r}^{\left(3\right)} & \Rightarrow & \nabla_{\mathbf{w}_{L}^{\left(2\right)}}E_{i}=\mathbf{r}_{L}^{\left(2\right)}\left(\mathbf{z}_{L}^{\left(1\right)}\right)^{T}\\
\mathbf{r}_{R}^{\left(2\right)}=\mathbf{a}_{LR}^{\left(2\right)}\sigma\left(\mathbf{a}_{L}^{\left(2\right)}\right)\sigma'\left(\mathbf{a}_{R}^{\left(2\right)}\right)\mathbf{w}^{\left(3\right)}\mathbf{r}^{\left(3\right)} & \Rightarrow & \mathbf{\nabla}_{\mathbf{w}_{R}^{\left(2\right)}}E_{i}=\mathbf{r}_{R}^{\left(2\right)}\left(\mathbf{z}_{R}^{\left(1\right)}\right)^{T}\\
\mathbf{r}_{LR}^{\left(3\right)}=\sigma\left(\mathbf{a}_{L}^{\left(2\right)}\right)\sigma\left(\mathbf{a}_{R}^{\left(2\right)}\right)\mathbf{w}^{\left(3\right)}\mathbf{r}^{\left(3\right)} & \Rightarrow & \mathbf{\nabla}_{\mathbf{w}_{LR}^{\left(2\right)}}E_{i}=\mathbf{r}_{LR}^{\left(2\right)}\left[\mathbf{z}_{L}^{\left(1\right)}\,\mathbf{z}_{R}^{\left(1\right)}\right]^{T}\\
\mathbf{r}_{L}^{\left(1\right)}=\left(\textrm{diag}\, g'\left(\mathbf{a}_{L}^{\left(2\right)}\right)\right)\left(\left(\mathbf{w}_{L}^{\left(2\right)}\right)^{T}\mathbf{r}_{L}^{\left(2\right)}+\left(\mathbf{w}_{LR[L]}^{\left(2\right)}\right)^{T}\mathbf{r}_{LR}^{\left(2\right)}\right) & \Rightarrow & \mathbf{\nabla}_{\mathbf{w}_{L}^{\left(1\right)}}E_{i}=\mathbf{r}_{L}^{\left(1\right)}\left(\mathbf{x}_{L}\right)^{T}\\
\mathbf{r}_{R}^{\left(1\right)}=\left(\textrm{diag}\, g'\left(\mathbf{a}_{R}^{\left(2\right)}\right)\right)\left(\left(\mathbf{w}_{L}^{\left(2\right)}\right)^{T}\mathbf{r}_{L}^{\left(2\right)}+\left(\mathbf{w}_{LR[R]}^{\left(2\right)}\right)^{T}\mathbf{r}_{LR}^{\left(2\right)}\right) & \Rightarrow & \mathbf{\nabla}_{\mathbf{w}_{R}^{\left(1\right)}}E_{i}=\mathbf{r_{R}^{\left(1\right)}}\left(\mathbf{x}_{R}\right)^{T}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Tabular
<lyxtabular version="3" rows="4" columns="2">
<features tabularvalignment="middle">
<column alignment="center" valignment="top" width="0pt">
<column alignment="left" valignment="top" width="0pt">
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Notation
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Description
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $\mathbf{w}_{LR[L]}^{\left(2\right)}$
\end_inset


\series medium
,
\series default
 
\begin_inset Formula $\mathbf{w}_{LR[R]}^{\left(2\right)}$
\end_inset

 
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
portion of the weight matrix corresponding to the left, respectively right,
 image
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $g\left(\cdot\right)$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
transfer function 
\begin_inset Formula $tanh\left(\cdot\right)$
\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $\left[\mathbf{z}_{L}^{\left(1\right)}\,\mathbf{z}_{R}^{\left(1\right)}\right]$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
concatenation of 
\begin_inset Formula $\mathbf{z}_{L}^{\left(1\right)}$
\end_inset

 and 
\begin_inset Formula $\mathbf{z}_{R}^{\left(1\right)}$
\end_inset

.
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\begin_layout Standard
\begin_inset VSpace smallskip
\end_inset


\end_layout

\begin_layout Standard
The residuals are computed using the derivative of the 
\begin_inset Formula $tanh$
\end_inset

 tranfer function, and the derivative of the logistic loss function.
 Then the gradients at each layer and for each stereo image (left, right)
 are computed using the residuals.
\end_layout

\begin_layout Paragraph
Multi-way classification
\end_layout

\begin_layout Standard
Most of the MLP we implemented for binary classification could be reused
 for multi-way classification.
 Only two changes were necessary :
\end_layout

\begin_layout Enumerate
The computation of the error on the top-layer had to be modified to squared
 error on 
\emph on
1 of K
\emph default
 coded target vector 
\series bold
\emph on
t
\series default
\emph default
, which implied the following change to the top-layer residual :
\begin_inset Formula 
\[
\mathbf{r}^{\left(3\right)}=\mathbf{a}^{\left(3\right)}-\mathbf{\tilde{t}}
\]

\end_inset


\end_layout

\begin_layout Enumerate
Since we now have 5 outputs instead of a single one, the computation of
 the class associated with the input image is becomes 
\begin_inset Formula $\underset{i=1,...,5}{argmax}\left(\mathbf{a}_{i}^{\left(3\right)}\right)$
\end_inset

 instead of 
\begin_inset Formula $sign\left(a^{\left(3\right)}\right)$
\end_inset

.
\end_layout

\begin_layout Standard
Interestingly, the equations for backpropagation don't need to be changed
 as they are vectorized and automatically take into account the multi-dimensiona
lity of the labels 
\begin_inset Formula $\mathbf{\tilde{t}}$
\end_inset

.
\end_layout

\begin_layout Paragraph
Model selection
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement h
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\begin_inset Box Frameless
position "t"
hor_pos "c"
has_inner_box 1
inner_pos "t"
use_parbox 0
use_makebox 0
width "45col%"
special "none"
height "1in"
height_special "totalheight"
status open

\begin_layout Plain Layout
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename comparative_mlp2.pdf
	width 100col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
Comparative plot showing the effect of the MLP and gradient descent parameters
\begin_inset CommandInset label
LatexCommand label
name "fig:Comparative-plot-showing"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset space \hfill{}
\end_inset


\begin_inset Box Frameless
position "t"
hor_pos "c"
has_inner_box 1
inner_pos "t"
use_parbox 0
use_makebox 0
width "45col%"
special "none"
height "1in"
height_special "totalheight"
status open

\begin_layout Plain Layout
\begin_inset Float figure
placement h
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename errors_mlp5_overfitting.pdf
	width 100col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
Example of overfitting for the multi-way MLP used for early-stopping
\begin_inset CommandInset label
LatexCommand label
name "fig:Example-of-overfitting"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
MLP training plots
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
Our MLP classifier had two native parameters (namely 
\begin_inset Formula $H_{1}$
\end_inset

 and 
\begin_inset Formula $H_{2}$
\end_inset

 which we considered seperately to ensure analysis completeness), and relies
 on gradient descent to converge to the optimal solution, which adds learning
 rate 
\begin_inset Formula $\eta$
\end_inset

, momentum term 
\begin_inset Formula $\mu$
\end_inset

 and mini-batch size to the parameter list.
 The impact of each parameter on the final performance of the MLP is unfortunate
ly extremely unintuitive and difficult to predict, especially because plausible
 ranges are highly inter-dependent.
 Because of this, we couldn't perform standard model selection by computing
 errors on the cartesian product of a reasonable range of parameter values.
\end_layout

\begin_layout Standard
We circumvented the problem by running the algorithm with reduced training
 and validation sets on a large combinations of values to get a reasonable
 estimation.
 This enabled us to narrow down the search space and perform better tests
 with the full training set.
 See Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Comparative-plot-showing"

\end_inset

 for examples of how the parameters influences the convergence of the MLP.
 For the learning rate 
\begin_inset Formula $\eta$
\end_inset

 we tested with constant values (in the order of 
\begin_inset Formula $10^{-2}$
\end_inset

 to 
\begin_inset Formula $10^{-5}$
\end_inset

) as well as variations of 
\begin_inset Formula $10^{-n}x^{-\frac{1}{a}}$
\end_inset

 with 
\begin_inset Formula $n\in\left[0;3\right]$
\end_inset

 and 
\begin_inset Formula $a\in\left[1;3\right]$
\end_inset

.
 The momentum term varied from 
\begin_inset Formula $0.05$
\end_inset

 to 
\begin_inset Formula $0.25$
\end_inset

, and the mini-batch size ranged from 
\begin_inset Formula $1$
\end_inset

 to 
\begin_inset Formula $50$
\end_inset

.
 As explained in the problem statement, we tested configuration of MLP with
 
\begin_inset Formula $1$
\end_inset

 to 
\begin_inset Formula $80$
\end_inset

 activation units in each hidden layers.
\end_layout

\begin_layout Standard
The optimal parameters for the binary MLP are 
\begin_inset Formula $\eta=10^{-3}$
\end_inset

, 
\begin_inset Formula $\mu=10^{-1}$
\end_inset

, with mini-batch size of 
\begin_inset Formula $20$
\end_inset

.
 The numbers of units in the first layer is 
\begin_inset Formula $20$
\end_inset

 and for the second layer is 
\begin_inset Formula $50$
\end_inset

.
 For the multi-way MLP, we found 
\begin_inset Formula $\eta=10^{-3}$
\end_inset

, 
\begin_inset Formula $\mu=10^{-1}$
\end_inset

, mini-batch size of 
\begin_inset Formula $5$
\end_inset

, and number of units in the first and second layer of 
\begin_inset Formula $60$
\end_inset

 and 
\begin_inset Formula $10$
\end_inset

 respectively.
\end_layout

\begin_layout Standard
In Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Example-of-overfitting"

\end_inset

 is an example of overfitting where the error on the validation set initially
 decreases but starts to increase as soon as the MLP is overfitting the
 training data.
 We used early stopping to avoid this behavior in all our gradient-descent
 based algorithms.
\end_layout

\begin_layout Subsection
Logistic regression
\end_layout

\begin_layout Standard
The logistic regression technique doesn't have any model parameters, but
 it relies on gradient descent which does (namely the learning rate 
\begin_inset Formula $\eta$
\end_inset

, momentum term 
\begin_inset Formula $\mu$
\end_inset

, and mini batch size).
 Unfortunately, as in the MLP case, it is difficult to perform standard
 model selection on these parameters.
\end_layout

\begin_layout Standard
After having manually determined plausible parameter combinations, we performed
 model selection by comparing convergence speed and final error rate on
 these values.
 During this process, we realized that logistic regression was extremely
 sensitive to initialization and training order, so we ran our model selection
 multiple times on each parameter combination and considered mean, best
 case and worst case performance instead of individual results when comparing
 models (see Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:comparative-logistic"

\end_inset

), resulting in the optimal configuration of 
\begin_inset Formula $\eta=2e^{-2}$
\end_inset

, 
\begin_inset Formula $\mu=5e^{-2}$
\end_inset

, with block size of 
\begin_inset Formula $5$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename logistic_selection.pdf
	width 100col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
Comparative plot of promising logistic regression model parameter combinations.
 The red and black curves both show good initial convergence and stability,
 but the red performs better in the tail.
 
\begin_inset CommandInset label
LatexCommand label
name "fig:comparative-logistic"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Least squares estimation
\end_layout

\begin_layout Paragraph
Error function
\end_layout

\begin_layout Standard
The analytical solution to the least squares estimation with Tikhonov regulariza
tion problem is obtained by solving the normal equations problem.
 In the case of multi-way classification, the solution is trivially extended
 by solving the equations for each of the k vectors obtained by using 
\emph on
1 of K
\emph default
 encoding for the target vector 
\series bold
\emph on
t
\series default
\emph default
.
 But this also means we multiply the impact of the Tikhonov regularizer
 by k.
 To compensate for this, we modified the regularized error function slightly
 :
\begin_inset Formula 
\[
E(\mathbf{W})=\frac{1}{2}\sum_{i=1}^{N}\parallel\mathbf{y}(\mathbf{x}_{i})-\mathbf{\tilde{t}}_{i}\parallel^{2}+\frac{1}{k}\frac{\nu}{2}\sum_{k=1}^{K}\parallel\mathbf{w}_{k}\parallel^{2}
\]

\end_inset


\end_layout

\begin_layout Paragraph
Model selection
\end_layout

\begin_layout Standard
The model selection process for the least squares estimation technique was
 by far the simplest since we were dealing with a single parameter and a
 constant convergence time.
 All computations were performed using 10-fold cross-validation, so we combined
 the 10 values obtained after each parameter validation run into boxplots
 for clarity.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement h
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename lstsq_interval.pdf
	width 80col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
Interval selection for Tikhonov regularization parameter.
 
\begin_inset CommandInset label
LatexCommand label
name "fig:Interval-selection-lstsq"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
We initially got a feeling for plausible values by setting the Tikhonov
 regularizer to the exponential range 
\begin_inset Formula $\left\{ 0\right\} \cup\left\{ 5\cdot10^{-2}*2^{k}\mid0\leq k<10\right\} $
\end_inset

 (see Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Interval-selection-lstsq"

\end_inset

).
 Once we had singled out a suitable interval, namely 
\begin_inset Formula $\left[0;1.2\right]$
\end_inset

, we performed a more precise search by resorting to a linear parameter
 space of 
\begin_inset Formula $\left\{ 5\cdot10^{-2}*k\mid0\leq k<25\right\} $
\end_inset

 (see Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Error-function-value"

\end_inset

) and we can thus place the optimal regularizer at 
\begin_inset Formula $0.6$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement h
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename lstsq_errors.pdf
	width 50col%

\end_inset


\begin_inset Graphics
	filename lstsq_classerrors.pdf
	width 50col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
Error function value (left) and classification error rate (right) during
 parameter selection for Tikhonov regularizer.
 We can clearly see the curve minimum around 0.5 (the actual minimum is at
 0.6).
 
\begin_inset CommandInset label
LatexCommand label
name "fig:Error-function-value"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Section
Results and Discussion
\end_layout

\begin_layout Subsection
Binary MLP and multi-way MLP
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement h
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename errors_mlp2.pdf
	width 80col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
Logistic error for binary classification using MLP for the training and
 the validation set.
 The classification error rate is not very interesting in this setting since
 it drops to zero after one run over all training points.
 
\begin_inset CommandInset label
LatexCommand label
name "fig:Logistic-error-for"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
The graph in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Logistic-error-for"

\end_inset

 shows the logistic error computed with the optimal parameters for the binary
 MLP.
 Both the errors for the training and validation sets are very close and
 the MLP converges quickly to an optimal solution, as both the classification
 error for the training and the validation set are null.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement h
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename errors_mlp5.pdf
	width 50col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename classerrors_mlp5.pdf
	width 50col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
Squared error (a) and classification error (b) for multi-way classification
 using MLP for the training and the validation set.
\begin_inset CommandInset label
LatexCommand label
name "fig:Logistic-error-for-1"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
The graph in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Logistic-error-for-1"

\end_inset

 shows the same metric for the multi-way MLP.
 The system converges slower than before and there is a better separation
 between the errors of the training and the validation set.
 We also showed a graph representing the classification error for both sets,
 as they both converge to zero but not at the same speed.
 It is interesting to notice that both the binary and multi-way MLP converge
 to a classification error of zero after enough training.
\end_layout

\begin_layout Subsection
Logistic regression
\end_layout

\begin_layout Standard
In Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Logistic-error-for-1-1"

\end_inset

, we show the error for the logistic regression technique in function of
 the training epoch.
 We see right away that the convergence speed is close to the one we witnessed
 in the MLP case, but the classification error rate of the validation set
 is much lower using the multi-way MLP technique.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement h
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename errors_logistic.pdf
	width 50col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename classerrors_logistic.pdf
	width 50col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
Logistic error (a) and classification error (b) for multi-way classification
 using logistic regression for the training and the validation set.
\begin_inset CommandInset label
LatexCommand label
name "fig:Logistic-error-for-1-1"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Classification results
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement h
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename testing_boxplot.pdf
	width 80col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
Comparative boxplot for each classifier
\begin_inset CommandInset label
LatexCommand label
name "fig:Comparative-boxplot-for"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
The results for the misclassification rate on the test set are shown in
 Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Comparative-boxplot-for"

\end_inset

.
 Both implementations of MLP show higher performance than linear classifiers.
 On the binary classification dataset, the binary MLP misclassified 
\begin_inset Formula $\mathbf{3.65}\%$
\end_inset

(
\series bold

\begin_inset Formula $\sigma=1.37$
\end_inset


\series default
).
 On the 5-way classification dataset, the MLP had an average error percentage
 of 
\series bold

\begin_inset Formula $\mathbf{11.29}\%$
\end_inset


\series default
 (
\begin_inset Formula $\sigma=1.11$
\end_inset

), the logistic regression performed worse with an average of 
\series bold

\begin_inset Formula $\mathbf{19.84}\%$
\end_inset


\series default
 (
\begin_inset Formula $\sigma=0.61$
\end_inset

) classification errors.
 All those results were computed on 10 runs.
 Linear regression with squared error had the worse classification error
 percentage with 
\begin_inset Formula $\mathbf{22.34}\%$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement h
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\begin_inset Graphics
	filename confusion_mlp2.pdf
	width 25col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
Binary MLP
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\begin_inset Graphics
	filename confusion_mlp5.pdf
	width 25col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
Multi-way MLP
\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\begin_inset Graphics
	filename confusion_logistic.pdf
	width 25col%

\end_inset

 
\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
Logistic error regression
\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\begin_inset Graphics
	filename confusion_lstsq.pdf
	width 25col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
Squared error regression
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
Confusion matrices (qualitative comparison)
\begin_inset CommandInset label
LatexCommand label
name "fig:Confusion-matrices"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
The confusion matrices are shown in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Confusion-matrices"

\end_inset

.
 As expected and confirmed by the boxplot in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Comparative-boxplot-for"

\end_inset

, the binary MLP performs very well with very few errors.
 The multi-class classification methods are more confused by the input images
 as the classes are closer to each other in terms of feature space.
 This qualitative comparison shows a decay in classification performance
 from the MLP to the squared error regression, confirming the results of
 Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Comparative-boxplot-for"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename bad_left.jpg
	scale 400

\end_inset


\begin_inset Graphics
	filename bad_right.jpg
	scale 400

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
Negative 
\begin_inset Formula $t_{i}a_{i}^{\left(3\right)}$
\end_inset

 close to zero
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename verybad_left.jpg
	scale 400

\end_inset


\begin_inset Graphics
	filename verybad_right.jpg
	scale 400

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
Large negative 
\begin_inset Formula $t_{i}a_{i}^{\left(3\right)}$
\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset Caption

\begin_layout Plain Layout
Misclassified image examples with the multi-way MLP
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\end_body
\end_document
